import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def create_data(num_samples, shuffle=False):
    x_values = np.linspace(-1, 1, num_samples)
    y_values = 0.2 * x_values ** 4 + 2 * x_values ** 3 + 0.1 * x_values ** 2 + 10
    data = np.column_stack((x_values, y_values))
    if shuffle:
        np.random.shuffle(data)
    return data

def train_val_test_split(data, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):
    assert train_ratio + val_ratio + test_ratio == 1.0, 'Ratios must sum to 1.0'
  
    num_train = int(train_ratio * len(data))
    num_val = int(val_ratio * len(data))
    num_test = len(data) - num_train - num_val

    train_data = data[:num_train]
    val_data = data[num_train:num_train+num_val]
    test_data = data[num_train+num_val:]

    return train_data, val_data, test_data

def scale_data(data):
    min_vals = np.min(data, axis=0)
    max_vals = np.max(data, axis=0)
    min_vals_minus = min_vals - 1
    apple = data - (min_vals_minus)
    apple1 = max_vals - (min_vals_minus)
    return (apple + 1) / (apple1 + 1)

def calculate_regression_metrics(actual, predicted):
    mae = mean_absolute_error(actual, predicted)
    mse = mean_squared_error(actual, predicted)
    rmse = np.sqrt(mse)
    r2 = r2_score(actual, predicted)
    return mae, mse, rmse, r2

# Generate shuffled and unshuffled data
num_samples = 30000
shuffled_data = create_data(num_samples, shuffle=True)
unshuffled_data = create_data(num_samples, shuffle=False)

# Split shuffled and unshuffled data into train, validation, and test sets
train_data, val_data, test_data = train_val_test_split(shuffled_data)
train_data2, val_data2, test_data2 = train_val_test_split(unshuffled_data)

# Scale the shuffled data
scaled_data = scale_data(shuffled_data)

# Plot shuffled, unshuffled, and scaled data
for data, title in zip([shuffled_data, unshuffled_data, scaled_data], ['Shuffled Data', 'Unshuffled Data', 'Scaled Data']):
    plt.scatter(data[:, 0], data[:, 1], s=0.1)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title(title)
    plt.show()

# Example usage of calculate_regression_metrics function
actual = [1, 2, 3, 4, 5]
predicted = [1.5, 2.5, 3.5, 4.5, 5.5]
mae, mse, rmse, r2 = calculate_regression_metrics(actual, predicted)
print(f"MAE: {mae:.4f}")
print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"R^2: {r2:.4f}")
print("R2 score: {:.4f}".format(r2))
------------------------------------------------
------------------------------------------------
---------------------------------------------
---------------------------------------------
-------------------------------------------
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

def create_data(num_samples, shuffle=False):
    x_values = np.linspace(-1, 1, num_samples)
    y_values = 0.2*x_values**4 + 2*x_values**3 + 0.1*x_values**2 + 10
    data = np.column_stack((x_values, y_values))
    if shuffle:
        np.random.shuffle(data)
    return data

def train_val_test_split(data, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):
    assert train_ratio + val_ratio + test_ratio == 1.0, 'Ratios must sum to 1.0'

    num_train = int(train_ratio * len(data))
    train_data = data[:num_train]
    remaining_data = data[num_train:]

    num_val = int(val_ratio / (val_ratio + test_ratio) * len(remaining_data))
    val_data = remaining_data[:num_val]
    test_data = remaining_data[num_val:]

    return train_data, val_data, test_data

def scale_data(data):
    apple2 = data - np.min(data, axis=0) - 1
    apple3 = np.max(data, axis=0) - np.min(data, axis=0) - 1
    return (apple2 + 1) / (apple3 + 1)
#-----------------------------Case 1----------------------------------------
# Setup
num_samples = 30000
shuffled_data = create_data(num_samples, shuffle=True)
unshuffled_data = create_data(num_samples, shuffle=False)
train_data, val_data, test_data = train_val_test_split(shuffled_data, train_ratio=0.3, val_ratio=0.2, test_ratio=0.5)
train_data2, val_data2, test_data2 = train_val_test_split(unshuffled_data, train_ratio=0.3, val_ratio=0.2, test_ratio=0.5)
scaled_data = scale_data(shuffled_data)
num_epochs = 20
batch_size = 12
loss_function = keras.losses.MeanSquaredError()
optimizer = keras.optimizers.Adam()

# Define model 1 - Structure 1 with Relu activation
model1 = keras.Sequential([
    keras.layers.Dense(units=12, activation='relu', input_shape=(1,)),
    keras.layers.Dense(units=8, activation='relu'),
    keras.layers.Dense(units=4, activation='relu'),
    keras.layers.Dense(units=1)
])
model1.compile(loss=loss_function, optimizer=optimizer)
history1 = model1.fit(train_data[:, 0], train_data[:, 1], epochs=num_epochs, batch_size=batch_size,
                      validation_data=(val_data[:, 0], val_data[:, 1]))
y_pred1 = model1.predict(test_data[:, 0])
mse1 = mean_squared_error(test_data[:, 1], y_pred1)
mae1 = mean_absolute_error(test_data[:, 1], y_pred1)
rmse1 = np.sqrt(mse1)
r2_score1 = r2_score(test_data[:, 1], y_pred1)

# Evaluate model 2 with shuffled and scaled data
y_pred2 = model1.predict(scale_data(test_data[:, 0]))
mse2 = mean_squared_error(test_data[:, 1], y_pred2)
mae2 = mean_absolute_error(test_data[:, 1], y_pred2)
rmse2 = np.sqrt(mse2)
r2_score2 = r2_score(test_data[:, 1], y_pred2)
y_pred2 = model1.predict(test_data[:, 0])
plt.scatter(test_data[:, 0], test_data[:, 1], label='Actual')
plt.scatter(test_data[:, 0], y_pred2, label='Predicted')
plt.legend()
plt.show()

#Print metrics for model 2
print('MAE of model 2:', mae2)
print('MSE of model 2:', mse2)
print('RMSE of model 2:', rmse2)
print('R2 score of model 2:', r2_score2)

#--------------------------Case 2---------------------------------
# Setup
num_samples = 30000
shuffled_data = create_data(num_samples, shuffle=True)
train_data, val_data, test_data = train_val_test_split(shuffled_data, train_ratio=0.3, val_ratio=0.2, test_ratio=0.5)
x_train = train_data[:, 0]
y_train = train_data[:, 1]
x_val = val_data[:, 0]
y_val = val_data[:, 1]
x_test = test_data[:, 0]
y_test = test_data[:, 1]
num_epochs = 20
batch_size = 12
loss_fn = tf.keras.losses.MSE

# Define the optimizer
optimizer = tf.keras.optimizers.Adam()
model = tf.keras.Sequential([
    tf.keras.layers.Dense(24, activation='relu', input_shape=(1,)),
    tf.keras.layers.Dense(1)
])

# Compile the model
model.compile(optimizer=optimizer, loss=loss_fn)
history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=num_epochs, batch_size=batch_size)
y_pred = model.predict(x_test).flatten()

# Calculate regression metrics
y_pred = model.predict(scale_data(test_data[:,0]))
mse = mean_squared_error(test_data[:,1], y_pred)
mae = mean_absolute_error(test_data[:,1], y_pred)
rmse = np.sqrt(mse)
r2_score = r2_score(test_data[:,1], y_pred)

print(f'MAE: {mae:.3f}, MSE: {mse:.3f}, RMSE: {rmse:.3f}, R2 score: {r2:.3f}')
plt.scatter(x_test, y_test, s=0.1, label='Actual')
plt.scatter(x_test, y_pred, s=0.1, label='Predicted')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Shuffled and unscaled data, Structure 2, Relu activation function')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and validation loss curves')
plt.legend()
plt.show()

#----------------------------Case 3-------------------------------------
# Setup
num_samples = 30000
shuffled_data = create_data(num_samples, shuffle=True)
unshuffled_data = create_data(num_samples, shuffle=False)
train_data, val_data, test_data = train_val_test_split(shuffled_data, train_ratio=0.3, val_ratio=0.2, test_ratio=0.5)
train_data2, val_data2, test_data2 = train_val_test_split(unshuffled_data, train_ratio=0.3, val_ratio=0.2, test_ratio=0.5)
epochs = 20
batch_size = 12
loss_function = 'mse'
optimizer = 'adam'

# Define the model
model = Sequential()
model.add(Dense(12, input_dim=1, activation='tanh'))
model.add(Dense(8, activation='tanh'))
model.add(Dense(4, activation='tanh'))
model.add(Dense(1))
model.compile(loss=loss_function, optimizer=optimizer, metrics=['mae', 'mse'])
model.compile(loss=loss_function, optimizer=optimizer, metrics=['mean_absolute_error'])
model.compile(loss=loss_function, optimizer=optimizer, metrics=['mean_squared_error'])
history = model.fit(train_data[:, 0], train_data[:, 1], epochs=epochs, batch_size=batch_size, validation_data=(val_data[:, 0], val_data[:, 1]))
test_loss, test_mse = model.evaluate(test_data[:, 0], test_data[:, 1])
print("Test Loss:", test_loss)
print("Test MSE:", test_mse)
predictions = model.predict(test_data[:, 0])

# Plot actual vs. predicted
plt.scatter(test_data[:, 0], test_data[:, 1], s=0.1, label='Actual')
plt.scatter(test_data[:, 0], predictions, s=0.1, label='Predicted')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Actual vs. Predicted (Case 3)')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss/Metric')
plt.title('Loss and Metrics over Epochs (Case 3)')
plt.legend()
plt.show()

# -----------------------------Case 4---------------------------------------
# Setup
data = create_data(num_samples=1000, shuffle=True)
train_data, val_data, test_data = train_val_test_split(data, train_ratio=0.3, val_ratio=0.2, test_ratio=0.5)
x_train, y_train = train_data[:, 0], train_data[:, 1]
x_val, y_val = val_data[:, 0], val_data[:, 1]
x_test, y_test = test_data[:, 0], test_data[:, 1]
x_train = scale_data(x_train)
y_train = scale_data(y_train)
x_val = scale_data(x_val)
y_val = scale_data(y_val)
x_test = scale_data(x_test)
y_test = scale_data(y_test)

# Define model
model = Sequential([
    Dense(10, activation='relu', input_shape=(1,)),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')
history = model.fit(x_train, y_train, epochs=20, batch_size=12, validation_data=(x_val, y_val))
y_pred = model.predict(x_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Print metrics
print("MSE:", mse)
print("MAE:", mae)

# Plot training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

plt.scatter(x_test, y_test, label='Actual Values')
plt.scatter(x_test, y_pred, label='Predicted Values')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Actual vs Predicted Values')
plt.legend()
plt.show()

# --------------------------Case 5----------------------------------------
# Setup
tf.random.set_seed(42)
np.random.seed(42)
model = Sequential([
    Dense(20, input_shape=(1,), activation='tanh'),
    Dense(20, activation='tanh'),
    Dense(1)
])
num_samples = 1000
data = create_data(num_samples, shuffle=True)

# Compile model
model.compile(loss='mse', optimizer='adam')
train_data, val_data, test_data = train_val_test_split(data, train_ratio=0.3, val_ratio=0.2, test_ratio=0.5)
train_data_scaled = scale_data(train_data)
val_data_scaled = scale_data(val_data)
test_data_scaled = scale_data(test_data)

# Train model
history = model.fit(train_data_scaled[:, 0], train_data_scaled[:, 1],
                    validation_data=(val_data_scaled[:, 0], val_data_scaled[:, 1]),
                    epochs=20, batch_size=12)

y_pred_scaled = model.predict(test_data_scaled[:, 0])
y_test_scaled = test_data_scaled[:, 1]
X_test_unscaled = test_data[:, 0].reshape(-1, 1)
y_test_unscaled = test_data[:, 1].reshape(-1, 1)
y_pred_unscaled = y_pred_scaled * (np.max(y_test_unscaled) - np.min(y_test_unscaled)) + np.min(y_test_unscaled)

mae = mean_absolute_error(y_test_unscaled, y_pred_unscaled)
mse = mean_squared_error(y_test_unscaled, y_pred_unscaled)
rmse = np.sqrt(mse)

# Print metrics
print(f'MAE: {mae:.2f}')
print(f'MSE: {mse:.2f}')
print(f'RMSE: {rmse:.2f}')

# Create plots
fig, ax = plt.subplots(2, 1, figsize=(8, 10))
ax[0].scatter(X_test_unscaled, y_test_unscaled, label='Actual')
ax[0].scatter(X_test_unscaled, y_pred_unscaled, label='Predicted')
ax[0].set_xlabel('X Test')
ax[0].set_ylabel('Y')
ax[0].set_title('Actual vs. Predicted')
ax[0].legend()

ax[1].scatter(X_test_unscaled, y_test_unscaled - y_pred_unscaled, label='Residuals')
ax[1].set_xlabel('X Test')
ax[1].set_ylabel('Residuals')
ax[1].set_title('Residuals')
ax[1].legend()

plt.tight_layout()
plt.show()

------------------------------------------------
------------------------------------------------
---------------------------------------------
---------------------------------------------
-------------------------------------------

import numpy as np
from keras.models import Sequential
from keras.layers import Dense

# Create the dataset
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# Define the neural network model
model = Sequential()
model.add(Dense(8, input_dim=2, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X, y, epochs=300, batch_size=4)

# Test the model
predictions = model.predict(X)
rounded_predictions = np.round(predictions)
print(rounded_predictions)
